{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e3d8a4",
   "metadata": {},
   "source": [
    "# EEG DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e016ab",
   "metadata": {},
   "source": [
    "The dimensions of the training set are as follows: 4,500 samples, 64 channels, and a time length of 795. This corresponds to 5 categories in y_train.\n",
    "\n",
    "The dimensions of the testing set are as follows: 750 samples, 64 channels, and a time length of 795. This corresponds to 5 categories in y_test.\n",
    "\n",
    "You can download it from this Google Drive link: [https://drive.google.com/drive/folders/1ykR-mn4d4KfFeeNrfR6UdtebsNRY8PU2?usp=sharing]. \n",
    "Please download the data and place it in your data_path at \"./data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b284602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef2ddad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Dataset not found. Please download the dataset from 'https://drive.google.com/drive/folders/1ykR-mn4d4KfFeeNrfR6UdtebsNRY8PU2?usp=sharing'\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Please download the dataset from 'https://drive.google.com/drive/folders/1ykR-mn4d4KfFeeNrfR6UdtebsNRY8PU2?usp=sharing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: Dataset not found. Please download the dataset from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://drive.google.com/drive/folders/1ykR-mn4d4KfFeeNrfR6UdtebsNRY8PU2?usp=sharing\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease download the dataset from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://drive.google.com/drive/folders/1ykR-mn4d4KfFeeNrfR6UdtebsNRY8PU2?usp=sharing\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Please download the dataset from 'https://drive.google.com/drive/folders/1ykR-mn4d4KfFeeNrfR6UdtebsNRY8PU2?usp=sharing'"
     ]
    }
   ],
   "source": [
    "data_path = './data/'\n",
    "if not os.path.exists(data_path + 'train_data.npy') or not os.path.exists(data_path + 'test_data.npy') or not os.path.exists(data_path + 'train_label.npy') or not os.path.exists(data_path + 'test_label.npy'):\n",
    "        print(\"ERROR: Dataset not found. Please download the dataset from 'https://drive.google.com/drive/folders/1ykR-mn4d4KfFeeNrfR6UdtebsNRY8PU2?usp=sharing'\")\n",
    "        print(\"\\n\\n\")\n",
    "        raise FileNotFoundError(\"Please download the dataset from 'https://drive.google.com/drive/folders/1ykR-mn4d4KfFeeNrfR6UdtebsNRY8PU2?usp=sharing'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb56c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(data_path + 'train_data.npy')\n",
    "test_data = np.load(data_path + 'test_data.npy')\n",
    "train_label = np.load(data_path + 'train_label.npy')\n",
    "test_label = np.load(data_path + 'test_label.npy')\n",
    "\n",
    "#To convert the data into PyTorch tensors\n",
    "x_train_tensor = torch.Tensor(train_data) \n",
    "y_train_tensor = torch.LongTensor(train_label) \n",
    "x_test_tensor = torch.Tensor(test_data)\n",
    "y_test_tensor = torch.LongTensor(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9369f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 3, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the training labels\n",
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f13109bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Cuda is available: \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #Setting GPU on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38430b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train_tensor.to(device), y_train_tensor.to(device)) # input data to Tensor dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, drop_last=True, shuffle=True) #  Batch size refers to the number of data sample\n",
    "test_dataset = TensorDataset(x_test_tensor.to(device), y_test_tensor.to(device))\n",
    "test_loader = DataLoader(test_dataset, batch_size=64,  drop_last=True,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304b4d6",
   "metadata": {},
   "source": [
    "# Build simple Deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f9468f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGAutoencoderClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_units=[256, 128, 64]):\n",
    "        super(EEGAutoencoderClassifier, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(64 * 795, hidden_units[0]), # Input dimention is 64 channel * 795 time point, and use 256 units for first NN layer\n",
    "            # nn.ReLU(), # Use ReLu function for NN training \n",
    "            nn.Softplus(),\n",
    "            nn.Linear(hidden_units[0], hidden_units[1]), # 256 NN units to 128 units\n",
    "            # nn.ReLU(),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(hidden_units[1], hidden_units[2]),#  128 NN units to 64 units\n",
    "            # nn.ReLU()\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_units[2], num_classes), # num_classes is 5 (hello,” “help me,” “stop,” “thank you,” and “yes”)\n",
    "            nn.LogSoftmax(dim=1)  # Use LogSoftmax for multi-class classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # import pdb;pdb.set_trace()\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2369fa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fzhao12/.cache/pypoetry/virtualenvs/bci-project-W8FRcYYV-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "num_classes = 5 # setting final output class\n",
    "model = EEGAutoencoderClassifier(num_classes, hidden_units=[1024, 512, 256]).to(device) \n",
    "criterion = nn.NLLLoss() # Use NLLLoss function to optimize\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001) # Setting parameters learning rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "086a7e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 1.660423755645752\n",
      "Epoch 2/500, Loss: 1.4420137405395508\n",
      "Epoch 3/500, Loss: 1.3515323400497437\n",
      "Epoch 4/500, Loss: 1.1079072952270508\n",
      "Epoch 5/500, Loss: 0.8526711463928223\n",
      "Epoch 6/500, Loss: 0.7015528082847595\n",
      "Epoch 7/500, Loss: 0.5894370079040527\n",
      "Epoch 8/500, Loss: 0.4076138436794281\n",
      "Epoch 9/500, Loss: 0.15064243972301483\n",
      "Epoch 10/500, Loss: 0.1570926308631897\n",
      "Epoch 11/500, Loss: 0.07666055858135223\n",
      "Epoch 12/500, Loss: 0.046869996935129166\n",
      "Epoch 13/500, Loss: 0.05492643266916275\n",
      "Epoch 14/500, Loss: 0.044283028692007065\n",
      "Epoch 15/500, Loss: 0.039010848850011826\n",
      "Epoch 16/500, Loss: 0.030360883101820946\n",
      "Epoch 17/500, Loss: 0.02121611498296261\n",
      "Epoch 18/500, Loss: 0.03355458006262779\n",
      "Epoch 19/500, Loss: 0.1021791398525238\n",
      "Epoch 20/500, Loss: 0.03338821604847908\n",
      "Epoch 21/500, Loss: 0.07286013662815094\n",
      "Epoch 22/500, Loss: 0.04369252920150757\n",
      "Epoch 23/500, Loss: 0.22485153377056122\n",
      "Epoch 24/500, Loss: 0.05284222960472107\n",
      "Epoch 25/500, Loss: 0.15010812878608704\n",
      "Epoch 26/500, Loss: 0.023836400359869003\n",
      "Epoch 27/500, Loss: 0.0017652750248089433\n",
      "Epoch 28/500, Loss: 0.002711137291043997\n",
      "Epoch 29/500, Loss: 0.01701032556593418\n",
      "Epoch 30/500, Loss: 0.0009541966137476265\n",
      "Epoch 31/500, Loss: 0.000919419398996979\n",
      "Epoch 32/500, Loss: 0.001171946874819696\n",
      "Epoch 33/500, Loss: 0.0005113401566632092\n",
      "Epoch 34/500, Loss: 0.0006951607065275311\n",
      "Epoch 35/500, Loss: 0.0005477105150930583\n",
      "Epoch 36/500, Loss: 0.0005090918275527656\n",
      "Epoch 37/500, Loss: 0.0005561045836657286\n",
      "Epoch 38/500, Loss: 0.0002813873579725623\n",
      "Epoch 39/500, Loss: 0.0003116033039987087\n",
      "Epoch 40/500, Loss: 0.00025761083816178143\n",
      "Epoch 41/500, Loss: 0.0002823595132213086\n",
      "Epoch 42/500, Loss: 0.0003116016450803727\n",
      "Epoch 43/500, Loss: 0.00028513450524769723\n",
      "Epoch 44/500, Loss: 0.000170644634636119\n",
      "Epoch 45/500, Loss: 0.0002034499339060858\n",
      "Epoch 46/500, Loss: 0.00018502540478948504\n",
      "Epoch 47/500, Loss: 0.00025740498676896095\n",
      "Epoch 48/500, Loss: 0.2852313816547394\n",
      "Epoch 49/500, Loss: 0.3873363137245178\n",
      "Epoch 50/500, Loss: 0.1607973873615265\n",
      "Epoch 51/500, Loss: 0.1573551595211029\n",
      "Epoch 52/500, Loss: 0.008361659944057465\n",
      "Epoch 53/500, Loss: 0.05187612771987915\n",
      "Epoch 54/500, Loss: 0.008240901865065098\n",
      "Epoch 55/500, Loss: 0.003301177406683564\n",
      "Epoch 56/500, Loss: 0.0017664710758253932\n",
      "Epoch 57/500, Loss: 0.004639152903109789\n",
      "Epoch 58/500, Loss: 0.003129015676677227\n",
      "Epoch 59/500, Loss: 0.0006920622545294464\n",
      "Epoch 60/500, Loss: 0.0011626831255853176\n",
      "Epoch 61/500, Loss: 0.0007236340316012502\n",
      "Epoch 62/500, Loss: 0.06073913723230362\n",
      "Epoch 63/500, Loss: 0.011252724565565586\n",
      "Epoch 64/500, Loss: 0.029345348477363586\n",
      "Epoch 65/500, Loss: 0.22580572962760925\n",
      "Epoch 66/500, Loss: 0.029848041012883186\n",
      "Epoch 67/500, Loss: 0.12019522488117218\n",
      "Epoch 68/500, Loss: 0.005957306362688541\n",
      "Epoch 69/500, Loss: 0.004318343009799719\n",
      "Epoch 70/500, Loss: 0.022758956998586655\n",
      "Epoch 71/500, Loss: 0.001139951404184103\n",
      "Epoch 72/500, Loss: 0.0011830405564978719\n",
      "Epoch 73/500, Loss: 0.0003598366165533662\n",
      "Epoch 74/500, Loss: 0.0004083068633917719\n",
      "Epoch 75/500, Loss: 0.0006000552675686777\n",
      "Epoch 76/500, Loss: 0.00023189824423752725\n",
      "Epoch 77/500, Loss: 0.00025882426416501403\n",
      "Epoch 78/500, Loss: 0.0003119158500339836\n",
      "Epoch 79/500, Loss: 0.00030182962655089796\n",
      "Epoch 80/500, Loss: 0.00019118816999252886\n",
      "Epoch 81/500, Loss: 0.0002278308675158769\n",
      "Epoch 82/500, Loss: 0.0006383624859154224\n",
      "Epoch 83/500, Loss: 0.00017442138050682843\n",
      "Epoch 84/500, Loss: 0.00015778520901221782\n",
      "Epoch 85/500, Loss: 0.00012344818969722837\n",
      "Epoch 86/500, Loss: 0.0001089790603145957\n",
      "Epoch 87/500, Loss: 0.00022916938178241253\n",
      "Epoch 88/500, Loss: 0.00010083689994644374\n",
      "Epoch 89/500, Loss: 0.00016437187150586396\n",
      "Epoch 90/500, Loss: 0.0005598616553470492\n",
      "Epoch 91/500, Loss: 0.000123729623737745\n",
      "Epoch 92/500, Loss: 9.457212581764907e-05\n",
      "Epoch 93/500, Loss: 6.838092667749152e-05\n",
      "Epoch 94/500, Loss: 7.310180808417499e-05\n",
      "Epoch 95/500, Loss: 7.782524335198104e-05\n",
      "Epoch 96/500, Loss: 4.7001016355352476e-05\n",
      "Epoch 97/500, Loss: 6.131331610959023e-05\n",
      "Epoch 98/500, Loss: 8.084597357083112e-05\n",
      "Epoch 99/500, Loss: 5.6971184676513076e-05\n",
      "Epoch 100/500, Loss: 7.82773204264231e-05\n",
      "Epoch 101/500, Loss: 4.925379471387714e-05\n",
      "Epoch 102/500, Loss: 5.576842522714287e-05\n",
      "Epoch 103/500, Loss: 5.773802331532352e-05\n",
      "Epoch 104/500, Loss: 3.7493919080588967e-05\n",
      "Epoch 105/500, Loss: 4.676624303101562e-05\n",
      "Epoch 106/500, Loss: 3.318126255180687e-05\n",
      "Epoch 107/500, Loss: 5.4734660807298496e-05\n",
      "Epoch 108/500, Loss: 3.8898095226613805e-05\n",
      "Epoch 109/500, Loss: 3.057751018786803e-05\n",
      "Epoch 110/500, Loss: 5.2200724894646555e-05\n",
      "Epoch 111/500, Loss: 0.00013784777547698468\n",
      "Epoch 112/500, Loss: 3.305379141238518e-05\n",
      "Epoch 113/500, Loss: 3.1074923754204065e-05\n",
      "Epoch 114/500, Loss: 2.496407432772685e-05\n",
      "Epoch 115/500, Loss: 2.2898420866113156e-05\n",
      "Epoch 116/500, Loss: 1.8845837985281833e-05\n",
      "Epoch 117/500, Loss: 1.7340737031190656e-05\n",
      "Epoch 118/500, Loss: 3.578831456252374e-05\n",
      "Epoch 119/500, Loss: 2.6496363716432825e-05\n",
      "Epoch 120/500, Loss: 2.9252962121972814e-05\n",
      "Epoch 121/500, Loss: 1.2902281014248729e-05\n",
      "Epoch 122/500, Loss: 4.111666203243658e-05\n",
      "Epoch 123/500, Loss: 2.847647738235537e-05\n",
      "Epoch 124/500, Loss: 1.066160712070996e-05\n",
      "Epoch 125/500, Loss: 1.1987737707386259e-05\n",
      "Epoch 126/500, Loss: 1.6355281331925653e-05\n",
      "Epoch 127/500, Loss: 1.3725208191317506e-05\n",
      "Epoch 128/500, Loss: 4.776609785039909e-05\n",
      "Epoch 129/500, Loss: 1.0838485650310759e-05\n",
      "Epoch 130/500, Loss: 1.9169485312886536e-05\n",
      "Epoch 131/500, Loss: 1.4490471585304476e-05\n",
      "Epoch 132/500, Loss: 1.2689833965850994e-05\n",
      "Epoch 133/500, Loss: 1.066534241545014e-05\n",
      "Epoch 134/500, Loss: 1.0810562343976926e-05\n",
      "Epoch 135/500, Loss: 1.2777518350048922e-05\n",
      "Epoch 136/500, Loss: 1.1900154277100228e-05\n",
      "Epoch 137/500, Loss: 1.6915328160393983e-05\n",
      "Epoch 138/500, Loss: 1.0300177564204205e-05\n",
      "Epoch 139/500, Loss: 9.780528671399225e-06\n",
      "Epoch 140/500, Loss: 8.281223017547745e-06\n",
      "Epoch 141/500, Loss: 6.7873725129175e-06\n",
      "Epoch 142/500, Loss: 8.37052175484132e-06\n",
      "Epoch 143/500, Loss: 1.0778105206554756e-05\n",
      "Epoch 144/500, Loss: 5.554291874432238e-06\n",
      "Epoch 145/500, Loss: 1.1160682333866134e-05\n",
      "Epoch 146/500, Loss: 9.408019650436472e-06\n",
      "Epoch 147/500, Loss: 5.3085027502675075e-06\n",
      "Epoch 148/500, Loss: 7.267964065249544e-06\n",
      "Epoch 149/500, Loss: 7.185997219494311e-06\n",
      "Epoch 150/500, Loss: 4.770158739120234e-06\n",
      "Epoch 151/500, Loss: 5.761120974057121e-06\n",
      "Epoch 152/500, Loss: 1.0752642083389219e-05\n",
      "Epoch 153/500, Loss: 6.763164947187761e-06\n",
      "Epoch 154/500, Loss: 4.040052317577647e-06\n",
      "Epoch 155/500, Loss: 5.1837009777955245e-06\n",
      "Epoch 156/500, Loss: 3.585568265407346e-06\n",
      "Epoch 157/500, Loss: 6.22674087935593e-06\n",
      "Epoch 158/500, Loss: 4.364130745670991e-06\n",
      "Epoch 159/500, Loss: 4.610004452842986e-06\n",
      "Epoch 160/500, Loss: 3.5036187000514474e-06\n",
      "Epoch 161/500, Loss: 3.248437224101508e-06\n",
      "Epoch 162/500, Loss: 4.751559572468977e-06\n",
      "Epoch 163/500, Loss: 4.203950993542094e-06\n",
      "Epoch 164/500, Loss: 7.221247415145626e-06\n",
      "Epoch 165/500, Loss: 2.7790508738689823e-06\n",
      "Epoch 166/500, Loss: 3.3322405670332955e-06\n",
      "Epoch 167/500, Loss: 4.233760591887403e-06\n",
      "Epoch 168/500, Loss: 4.025139332952676e-06\n",
      "Epoch 169/500, Loss: 4.468454335437855e-06\n",
      "Epoch 170/500, Loss: 3.2204973194893682e-06\n",
      "Epoch 171/500, Loss: 2.38603774960211e-06\n",
      "Epoch 172/500, Loss: 5.1501242523954716e-06\n",
      "Epoch 173/500, Loss: 2.5741674107848667e-06\n",
      "Epoch 174/500, Loss: 2.9057082429062575e-06\n",
      "Epoch 175/500, Loss: 2.361824044783134e-06\n",
      "Epoch 176/500, Loss: 2.2612443899561185e-06\n",
      "Epoch 177/500, Loss: 2.596515969344182e-06\n",
      "Epoch 178/500, Loss: 3.904063305526506e-06\n",
      "Epoch 179/500, Loss: 2.160657459171489e-06\n",
      "Epoch 180/500, Loss: 2.1085068055981537e-06\n",
      "Epoch 181/500, Loss: 4.824006282433402e-06\n",
      "Epoch 182/500, Loss: 2.0451766431506258e-06\n",
      "Epoch 183/500, Loss: 1.7955846942641074e-06\n",
      "Epoch 184/500, Loss: 1.4193309425536427e-06\n",
      "Epoch 185/500, Loss: 1.3560028264691937e-06\n",
      "Epoch 186/500, Loss: 1.886853169708047e-06\n",
      "Epoch 187/500, Loss: 1.5515809081989573e-06\n",
      "Epoch 188/500, Loss: 1.2535572295746533e-06\n",
      "Epoch 189/500, Loss: 1.4174650004861178e-06\n",
      "Epoch 190/500, Loss: 4.580021141009638e-06\n",
      "Epoch 191/500, Loss: 1.5813795926078456e-06\n",
      "Epoch 192/500, Loss: 1.173463942905073e-06\n",
      "Epoch 193/500, Loss: 1.4137435755401384e-06\n",
      "Epoch 194/500, Loss: 1.2442394563549897e-06\n",
      "Epoch 195/500, Loss: 9.611230780137703e-07\n",
      "Epoch 196/500, Loss: 1.0766061677713878e-06\n",
      "Epoch 197/500, Loss: 1.354138021270046e-06\n",
      "Epoch 198/500, Loss: 9.052438372236793e-07\n",
      "Epoch 199/500, Loss: 1.3466881227941485e-06\n",
      "Epoch 200/500, Loss: 1.3038458064329461e-06\n",
      "Epoch 201/500, Loss: 1.4603094768972369e-06\n",
      "Epoch 202/500, Loss: 5.718313218494586e-07\n",
      "Epoch 203/500, Loss: 7.152542593757971e-07\n",
      "Epoch 204/500, Loss: 1.0710188007578836e-06\n",
      "Epoch 205/500, Loss: 1.475211661272624e-06\n",
      "Epoch 206/500, Loss: 9.331828323411173e-07\n",
      "Epoch 207/500, Loss: 5.997710559313418e-07\n",
      "Epoch 208/500, Loss: 7.636833174728963e-07\n",
      "Epoch 209/500, Loss: 6.519248358927143e-07\n",
      "Epoch 210/500, Loss: 8.847544563650445e-07\n",
      "Epoch 211/500, Loss: 8.363262509192282e-07\n",
      "Epoch 212/500, Loss: 5.79282072976639e-07\n",
      "Epoch 213/500, Loss: 6.575132260877581e-07\n",
      "Epoch 214/500, Loss: 5.681062589246721e-07\n",
      "Epoch 215/500, Loss: 6.277104489527119e-07\n",
      "Epoch 216/500, Loss: 6.183977347973268e-07\n",
      "Epoch 217/500, Loss: 4.284079864191881e-07\n",
      "Epoch 218/500, Loss: 4.861500428887666e-07\n",
      "Epoch 219/500, Loss: 6.034960051692906e-07\n",
      "Epoch 220/500, Loss: 2.831219205745583e-07\n",
      "Epoch 221/500, Loss: 4.15369640904828e-07\n",
      "Epoch 222/500, Loss: 4.2468241190363187e-07\n",
      "Epoch 223/500, Loss: 3.967430757256807e-07\n",
      "Epoch 224/500, Loss: 4.82424638903467e-07\n",
      "Epoch 225/500, Loss: 5.234027184997103e-07\n",
      "Epoch 226/500, Loss: 3.4645177038328256e-07\n",
      "Epoch 227/500, Loss: 4.153695840614091e-07\n",
      "Epoch 228/500, Loss: 3.818419713752519e-07\n",
      "Epoch 229/500, Loss: 4.0605630147183547e-07\n",
      "Epoch 230/500, Loss: 2.7008343295165105e-07\n",
      "Epoch 231/500, Loss: 2.384184938364342e-07\n",
      "Epoch 232/500, Loss: 3.874294804973033e-07\n",
      "Epoch 233/500, Loss: 2.849845657237893e-07\n",
      "Epoch 234/500, Loss: 3.46451798804992e-07\n",
      "Epoch 235/500, Loss: 2.8125936069045565e-07\n",
      "Epoch 236/500, Loss: 3.669406964945665e-07\n",
      "Epoch 237/500, Loss: 2.961604081974656e-07\n",
      "Epoch 238/500, Loss: 2.849845941454987e-07\n",
      "Epoch 239/500, Loss: 1.750885871842911e-07\n",
      "Epoch 240/500, Loss: 1.378357126213814e-07\n",
      "Epoch 241/500, Loss: 1.8998976258899347e-07\n",
      "Epoch 242/500, Loss: 3.1851192261456163e-07\n",
      "Epoch 243/500, Loss: 2.495941089364351e-07\n",
      "Epoch 244/500, Loss: 2.775340419702843e-07\n",
      "Epoch 245/500, Loss: 1.5832480926292192e-07\n",
      "Epoch 246/500, Loss: 1.6018745441215287e-07\n",
      "Epoch 247/500, Loss: 2.682207025372918e-07\n",
      "Epoch 248/500, Loss: 2.2724249504335603e-07\n",
      "Epoch 249/500, Loss: 1.8253919620292436e-07\n",
      "Epoch 250/500, Loss: 2.980227407078928e-07\n",
      "Epoch 251/500, Loss: 1.3224779138454323e-07\n",
      "Epoch 252/500, Loss: 1.4901149825163884e-07\n",
      "Epoch 253/500, Loss: 1.5459951896446e-07\n",
      "Epoch 254/500, Loss: 1.5832473820864834e-07\n",
      "Epoch 255/500, Loss: 1.1920923270736239e-07\n",
      "Epoch 256/500, Loss: 1.2852250108608132e-07\n",
      "Epoch 257/500, Loss: 1.695006233148888e-07\n",
      "Epoch 258/500, Loss: 9.126959810146218e-08\n",
      "Epoch 259/500, Loss: 1.5087418603343394e-07\n",
      "Epoch 260/500, Loss: 1.7508851613001752e-07\n",
      "Epoch 261/500, Loss: 1.0058282384761696e-07\n",
      "Epoch 262/500, Loss: 9.313223614526578e-08\n",
      "Epoch 263/500, Loss: 1.3224777717368852e-07\n",
      "Epoch 264/500, Loss: 9.872017869838601e-08\n",
      "Epoch 265/500, Loss: 6.332992796842518e-08\n",
      "Epoch 266/500, Loss: 1.3038513202445756e-07\n",
      "Epoch 267/500, Loss: 2.5890696520036727e-07\n",
      "Epoch 268/500, Loss: 8.940695295223122e-08\n",
      "Epoch 269/500, Loss: 8.940695295223122e-08\n",
      "Epoch 270/500, Loss: 8.195635814445268e-08\n",
      "Epoch 271/500, Loss: 1.1920926112907182e-07\n",
      "Epoch 272/500, Loss: 7.823108916227284e-08\n",
      "Epoch 273/500, Loss: 7.078050145992165e-08\n",
      "Epoch 274/500, Loss: 1.7136297003617074e-07\n",
      "Epoch 275/500, Loss: 5.4016702222270396e-08\n",
      "Epoch 276/500, Loss: 8.568166265376931e-08\n",
      "Epoch 277/500, Loss: 4.8428770327291204e-08\n",
      "Epoch 278/500, Loss: 5.0291408371094803e-08\n",
      "Epoch 279/500, Loss: 5.7741978309877595e-08\n",
      "Epoch 280/500, Loss: 5.774198541530495e-08\n",
      "Epoch 281/500, Loss: 6.332992796842518e-08\n",
      "Epoch 282/500, Loss: 3.166496398421259e-08\n",
      "Epoch 283/500, Loss: 6.146728281919422e-08\n",
      "Epoch 284/500, Loss: 5.029141547652216e-08\n",
      "Epoch 285/500, Loss: 5.029141547652216e-08\n",
      "Epoch 286/500, Loss: 2.9802320611338473e-08\n",
      "Epoch 287/500, Loss: 3.725289943190546e-08\n",
      "Epoch 288/500, Loss: 3.352760913344355e-08\n",
      "Epoch 289/500, Loss: 3.166496398421259e-08\n",
      "Epoch 290/500, Loss: 4.4703465817974575e-08\n",
      "Epoch 291/500, Loss: 4.284083487959833e-08\n",
      "Epoch 292/500, Loss: 5.960463766996327e-08\n",
      "Epoch 293/500, Loss: 1.098958577472331e-07\n",
      "Epoch 294/500, Loss: 3.166496398421259e-08\n",
      "Epoch 295/500, Loss: 2.2351740014414645e-08\n",
      "Epoch 296/500, Loss: 3.5390254282674505e-08\n",
      "Epoch 297/500, Loss: 2.7939675462107516e-08\n",
      "Epoch 298/500, Loss: 3.352760913344355e-08\n",
      "Epoch 299/500, Loss: 1.303851515643828e-08\n",
      "Epoch 300/500, Loss: 1.1175870007207322e-08\n",
      "Epoch 301/500, Loss: 1.1175870007207322e-08\n",
      "Epoch 302/500, Loss: 2.0489094865183688e-08\n",
      "Epoch 303/500, Loss: 1.303851515643828e-08\n",
      "Epoch 304/500, Loss: 2.607703031287656e-08\n",
      "Epoch 305/500, Loss: 5.587935003603661e-09\n",
      "Epoch 306/500, Loss: 2.607702853651972e-08\n",
      "Epoch 307/500, Loss: 2.2351740014414645e-08\n",
      "Epoch 308/500, Loss: 1.862644971595273e-08\n",
      "Epoch 309/500, Loss: 1.303851515643828e-08\n",
      "Epoch 310/500, Loss: 1.1175870007207322e-08\n",
      "Epoch 311/500, Loss: 1.862644971595273e-08\n",
      "Epoch 312/500, Loss: 1.303851426825986e-08\n",
      "Epoch 313/500, Loss: 3.725290076417309e-09\n",
      "Epoch 314/500, Loss: 2.2351740014414645e-08\n",
      "Epoch 315/500, Loss: 3.725290076417309e-09\n",
      "Epoch 316/500, Loss: 7.450580152834618e-09\n",
      "Epoch 317/500, Loss: 1.4901159417490817e-08\n",
      "Epoch 318/500, Loss: 5.58792727645141e-08\n",
      "Epoch 319/500, Loss: 3.725290076417309e-09\n",
      "Epoch 320/500, Loss: 1.1175870007207322e-08\n",
      "Epoch 321/500, Loss: 2.235172757991677e-08\n",
      "Epoch 322/500, Loss: 1.8626450382086546e-09\n",
      "Epoch 323/500, Loss: 5.587935003603661e-09\n",
      "Epoch 324/500, Loss: 3.725290076417309e-09\n",
      "Epoch 325/500, Loss: 7.450580152834618e-09\n",
      "Epoch 326/500, Loss: 1.6763804566721774e-08\n",
      "Epoch 327/500, Loss: 1.1175870007207322e-08\n",
      "Epoch 328/500, Loss: 7.450580152834618e-09\n",
      "Epoch 329/500, Loss: 7.450580152834618e-09\n",
      "Epoch 330/500, Loss: 9.313224857976365e-09\n",
      "Epoch 331/500, Loss: 1.1175870007207322e-08\n",
      "Epoch 332/500, Loss: 7.450580152834618e-09\n",
      "Epoch 333/500, Loss: 9.313224857976365e-09\n",
      "Epoch 334/500, Loss: 1.8626450382086546e-09\n",
      "Epoch 335/500, Loss: 1.8626450382086546e-09\n",
      "Epoch 336/500, Loss: 9.313224857976365e-09\n",
      "Epoch 337/500, Loss: 9.313224857976365e-09\n",
      "Epoch 338/500, Loss: 7.450580152834618e-09\n",
      "Epoch 339/500, Loss: 1.303851515643828e-08\n",
      "Epoch 340/500, Loss: 3.725290076417309e-09\n",
      "Epoch 341/500, Loss: 9.313224857976365e-09\n",
      "Epoch 342/500, Loss: 3.7252898543727042e-09\n",
      "Epoch 343/500, Loss: 9.313224857976365e-09\n",
      "Epoch 344/500, Loss: 1.1175870007207322e-08\n",
      "Epoch 345/500, Loss: 7.4505797087454084e-09\n",
      "Epoch 346/500, Loss: 9.313224857976365e-09\n",
      "Epoch 347/500, Loss: 1.303851515643828e-08\n",
      "Epoch 348/500, Loss: 9.313224857976365e-09\n",
      "Epoch 349/500, Loss: 1.1175870007207322e-08\n",
      "Epoch 350/500, Loss: 0.8185524344444275\n",
      "Epoch 351/500, Loss: 0.2693849802017212\n",
      "Epoch 352/500, Loss: 0.033974528312683105\n",
      "Epoch 353/500, Loss: 0.005365851800888777\n",
      "Epoch 354/500, Loss: 0.004061886575073004\n",
      "Epoch 355/500, Loss: 0.01514551229774952\n",
      "Epoch 356/500, Loss: 0.0014129603514447808\n",
      "Epoch 357/500, Loss: 0.001441167201846838\n",
      "Epoch 358/500, Loss: 0.002377535216510296\n",
      "Epoch 359/500, Loss: 0.0006853191298432648\n",
      "Epoch 360/500, Loss: 0.0008423051913268864\n",
      "Epoch 361/500, Loss: 0.00048609598889015615\n",
      "Epoch 362/500, Loss: 0.0007784453337080777\n",
      "Epoch 363/500, Loss: 0.0006924219778738916\n",
      "Epoch 364/500, Loss: 0.0005287682288326323\n",
      "Epoch 365/500, Loss: 0.0006351633928716183\n",
      "Epoch 366/500, Loss: 0.00035409623524174094\n",
      "Epoch 367/500, Loss: 0.0004577237996272743\n",
      "Epoch 368/500, Loss: 0.00044323879410512745\n",
      "Epoch 369/500, Loss: 0.00030609581153839827\n",
      "Epoch 370/500, Loss: 0.0002069765469059348\n",
      "Epoch 371/500, Loss: 0.0002444844867568463\n",
      "Epoch 372/500, Loss: 0.0002774413151200861\n",
      "Epoch 373/500, Loss: 0.00021601033222395927\n",
      "Epoch 374/500, Loss: 0.0001650554040679708\n",
      "Epoch 375/500, Loss: 0.0003066724166274071\n",
      "Epoch 376/500, Loss: 0.00016051957209128886\n",
      "Epoch 377/500, Loss: 0.00021698558703064919\n",
      "Epoch 378/500, Loss: 0.0001751153904478997\n",
      "Epoch 379/500, Loss: 0.00019718568364623934\n",
      "Epoch 380/500, Loss: 0.00018570487736724317\n",
      "Epoch 381/500, Loss: 0.0001471915456932038\n",
      "Epoch 382/500, Loss: 0.00015883773448877037\n",
      "Epoch 383/500, Loss: 0.00022255875228438526\n",
      "Epoch 384/500, Loss: 0.00013445499644149095\n",
      "Epoch 385/500, Loss: 0.00016841752221807837\n",
      "Epoch 386/500, Loss: 0.00013366593339014798\n",
      "Epoch 387/500, Loss: 0.00011140343849547207\n",
      "Epoch 388/500, Loss: 7.020345219643787e-05\n",
      "Epoch 389/500, Loss: 0.00010999957885360345\n",
      "Epoch 390/500, Loss: 6.534144631586969e-05\n",
      "Epoch 391/500, Loss: 8.656516729388386e-05\n",
      "Epoch 392/500, Loss: 5.9653855714714155e-05\n",
      "Epoch 393/500, Loss: 6.489127554232255e-05\n",
      "Epoch 394/500, Loss: 0.00010056082101073116\n",
      "Epoch 395/500, Loss: 8.630977390566841e-05\n",
      "Epoch 396/500, Loss: 9.992964623961598e-05\n",
      "Epoch 397/500, Loss: 0.00010938676132354885\n",
      "Epoch 398/500, Loss: 6.63728205836378e-05\n",
      "Epoch 399/500, Loss: 7.323452882701531e-05\n",
      "Epoch 400/500, Loss: 4.861637717112899e-05\n",
      "Epoch 401/500, Loss: 5.844934639753774e-05\n",
      "Epoch 402/500, Loss: 7.528741843998432e-05\n",
      "Epoch 403/500, Loss: 3.8306243368424475e-05\n",
      "Epoch 404/500, Loss: 5.069942199042998e-05\n",
      "Epoch 405/500, Loss: 3.6573132092598826e-05\n",
      "Epoch 406/500, Loss: 5.050872641731985e-05\n",
      "Epoch 407/500, Loss: 2.4360448151128367e-05\n",
      "Epoch 408/500, Loss: 4.4336760765872896e-05\n",
      "Epoch 409/500, Loss: 5.753247023676522e-05\n",
      "Epoch 410/500, Loss: 4.452591383596882e-05\n",
      "Epoch 411/500, Loss: 4.188877937849611e-05\n",
      "Epoch 412/500, Loss: 3.274900154792704e-05\n",
      "Epoch 413/500, Loss: 2.312744800292421e-05\n",
      "Epoch 414/500, Loss: 2.742035940173082e-05\n",
      "Epoch 415/500, Loss: 2.2531548893311992e-05\n",
      "Epoch 416/500, Loss: 3.0530642106896266e-05\n",
      "Epoch 417/500, Loss: 2.3144393708207645e-05\n",
      "Epoch 418/500, Loss: 2.346483051951509e-05\n",
      "Epoch 419/500, Loss: 2.891981421271339e-05\n",
      "Epoch 420/500, Loss: 2.117169788107276e-05\n",
      "Epoch 421/500, Loss: 1.9735825844691135e-05\n",
      "Epoch 422/500, Loss: 1.722709566820413e-05\n",
      "Epoch 423/500, Loss: 1.7355539966956712e-05\n",
      "Epoch 424/500, Loss: 1.713395795377437e-05\n",
      "Epoch 425/500, Loss: 1.556384449941106e-05\n",
      "Epoch 426/500, Loss: 1.695118953648489e-05\n",
      "Epoch 427/500, Loss: 1.9972607333329506e-05\n",
      "Epoch 428/500, Loss: 2.2341137082548812e-05\n",
      "Epoch 429/500, Loss: 2.3082857296685688e-05\n",
      "Epoch 430/500, Loss: 1.5366415027529e-05\n",
      "Epoch 431/500, Loss: 1.6755675460444763e-05\n",
      "Epoch 432/500, Loss: 1.1842466847156174e-05\n",
      "Epoch 433/500, Loss: 1.7087120795622468e-05\n",
      "Epoch 434/500, Loss: 9.534714990877546e-06\n",
      "Epoch 435/500, Loss: 1.0207081686530728e-05\n",
      "Epoch 436/500, Loss: 2.3497495931223966e-05\n",
      "Epoch 437/500, Loss: 1.1713897038134746e-05\n",
      "Epoch 438/500, Loss: 1.114951191993896e-05\n",
      "Epoch 439/500, Loss: 1.0598302651487757e-05\n",
      "Epoch 440/500, Loss: 1.179019636765588e-05\n",
      "Epoch 441/500, Loss: 8.698406418261584e-06\n",
      "Epoch 442/500, Loss: 1.0208974345005117e-05\n",
      "Epoch 443/500, Loss: 1.0879532055696473e-05\n",
      "Epoch 444/500, Loss: 5.561810667131795e-06\n",
      "Epoch 445/500, Loss: 1.152940694737481e-05\n",
      "Epoch 446/500, Loss: 7.08541983840405e-06\n",
      "Epoch 447/500, Loss: 5.682881237589754e-06\n",
      "Epoch 448/500, Loss: 6.403686256817309e-06\n",
      "Epoch 449/500, Loss: 5.626993242913159e-06\n",
      "Epoch 450/500, Loss: 7.411341357510537e-06\n",
      "Epoch 451/500, Loss: 1.0342521818529349e-05\n",
      "Epoch 452/500, Loss: 4.444246314960765e-06\n",
      "Epoch 453/500, Loss: 4.438649284566054e-06\n",
      "Epoch 454/500, Loss: 4.442375939106569e-06\n",
      "Epoch 455/500, Loss: 5.828139819641365e-06\n",
      "Epoch 456/500, Loss: 4.490811988944188e-06\n",
      "Epoch 457/500, Loss: 4.559732133202488e-06\n",
      "Epoch 458/500, Loss: 7.193192686827388e-06\n",
      "Epoch 459/500, Loss: 4.446103048394434e-06\n",
      "Epoch 460/500, Loss: 1.0286207725584973e-05\n",
      "Epoch 461/500, Loss: 5.640032668452477e-06\n",
      "Epoch 462/500, Loss: 5.569264430960175e-06\n",
      "Epoch 463/500, Loss: 3.8034872886782978e-06\n",
      "Epoch 464/500, Loss: 4.796251687366748e-06\n",
      "Epoch 465/500, Loss: 3.0454089028353337e-06\n",
      "Epoch 466/500, Loss: 3.9357437344733626e-06\n",
      "Epoch 467/500, Loss: 4.528063072939403e-06\n",
      "Epoch 468/500, Loss: 3.179517079843208e-06\n",
      "Epoch 469/500, Loss: 2.2388862817024346e-06\n",
      "Epoch 470/500, Loss: 3.1273671083908994e-06\n",
      "Epoch 471/500, Loss: 2.833069174812408e-06\n",
      "Epoch 472/500, Loss: 3.7029099075880367e-06\n",
      "Epoch 473/500, Loss: 2.6561210688669235e-06\n",
      "Epoch 474/500, Loss: 2.6803368200489786e-06\n",
      "Epoch 475/500, Loss: 3.2931372970779194e-06\n",
      "Epoch 476/500, Loss: 2.184877757827053e-06\n",
      "Epoch 477/500, Loss: 2.9019870453339536e-06\n",
      "Epoch 478/500, Loss: 2.4400521851930534e-06\n",
      "Epoch 479/500, Loss: 2.13644739233132e-06\n",
      "Epoch 480/500, Loss: 2.1159573861950776e-06\n",
      "Epoch 481/500, Loss: 1.907342493723263e-06\n",
      "Epoch 482/500, Loss: 1.3336509709915845e-06\n",
      "Epoch 483/500, Loss: 2.151348780898843e-06\n",
      "Epoch 484/500, Loss: 1.564617832627846e-06\n",
      "Epoch 485/500, Loss: 2.3860404780862154e-06\n",
      "Epoch 486/500, Loss: 1.6074595805548597e-06\n",
      "Epoch 487/500, Loss: 1.4677605122415116e-06\n",
      "Epoch 488/500, Loss: 1.7601960280444473e-06\n",
      "Epoch 489/500, Loss: 1.7061781818483723e-06\n",
      "Epoch 490/500, Loss: 1.2647336689042277e-06\n",
      "Epoch 491/500, Loss: 9.033815899783804e-07\n",
      "Epoch 492/500, Loss: 1.0021007028626627e-06\n",
      "Epoch 493/500, Loss: 7.431946187352878e-07\n",
      "Epoch 494/500, Loss: 9.052444625012868e-07\n",
      "Epoch 495/500, Loss: 1.0766066225187387e-06\n",
      "Epoch 496/500, Loss: 1.0933692919934401e-06\n",
      "Epoch 497/500, Loss: 8.549531571588886e-07\n",
      "Epoch 498/500, Loss: 1.0095525340148015e-06\n",
      "Epoch 499/500, Loss: 1.1082714763688273e-06\n",
      "Epoch 500/500, Loss: 1.4733484476892045e-06\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500 # setting training epochs (Number of training iterations)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data, labels in train_loader: \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b28c80b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    model.eval() # Evaluate your model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    # print the predited label and the true label\n",
    "    print(predicted)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7969f355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 57.81%\n",
      "tensor([2, 4, 0, 3, 0, 0, 3, 2, 2, 4, 3, 1, 3, 2, 0, 4, 4, 4, 3, 3, 4, 2, 2, 1,\n",
      "        3, 1, 3, 0, 2, 3, 1, 3, 2, 3, 4, 3, 4, 4, 3, 3, 3, 1, 3, 0, 2, 3, 2, 1,\n",
      "        1, 4, 0, 2, 2, 4, 0, 3, 2, 3, 2, 1, 2, 4, 4, 3], device='cuda:0')\n",
      "tensor([2, 4, 1, 4, 4, 1, 4, 2, 1, 0, 3, 0, 3, 2, 3, 4, 4, 4, 0, 4, 1, 0, 2, 1,\n",
      "        3, 1, 0, 4, 2, 3, 1, 2, 2, 0, 4, 3, 2, 4, 2, 0, 3, 3, 1, 0, 2, 3, 1, 0,\n",
      "        1, 4, 0, 2, 1, 4, 0, 1, 2, 3, 4, 1, 2, 4, 3, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model)\n",
    "# save the model with num_epochs suffix to 'trained_models' directory\n",
    "file_name = f'trained_models/eeg_autoencoder_classifier_{num_epochs}.pth'\n",
    "torch.save(model.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67dcd9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 57.81%\n",
      "tensor([2, 4, 0, 3, 0, 0, 3, 2, 2, 4, 3, 1, 3, 2, 0, 4, 4, 4, 3, 3, 4, 2, 2, 1,\n",
      "        3, 1, 3, 0, 2, 3, 1, 3, 2, 3, 4, 3, 4, 4, 3, 3, 3, 1, 3, 0, 2, 3, 2, 1,\n",
      "        1, 4, 0, 2, 2, 4, 0, 3, 2, 3, 2, 1, 2, 4, 4, 3], device='cuda:0')\n",
      "tensor([2, 4, 1, 4, 4, 1, 4, 2, 1, 0, 3, 0, 3, 2, 3, 4, 4, 4, 0, 4, 1, 0, 2, 1,\n",
      "        3, 1, 0, 4, 2, 3, 1, 2, 2, 0, 4, 3, 2, 4, 2, 0, 3, 3, 1, 0, 2, 3, 1, 0,\n",
      "        1, 4, 0, 2, 1, 4, 0, 1, 2, 3, 4, 1, 2, 4, 3, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# load the model from the file\n",
    "model = EEGAutoencoderClassifier(num_classes=5, hidden_units=[1024, 512, 256]).to(device)\n",
    "model.load_state_dict(torch.load(file_name))\n",
    "evaluate_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
